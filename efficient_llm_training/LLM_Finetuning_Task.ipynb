{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LufV608o36pC"
      },
      "source": [
        "# Efficient LLM Training Tutorial\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MENA-ML/tutorials2025-tasks/blob/main/efficient_llm_training/LLM_Finetuning_Task.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This is the tutorial for the **2025 Middle East and North Africa Machine Learning (MenaML) Winter School**!\n",
        "\n",
        "This tutorial will explore the fundamental aspects of Finetuning Large Language Models for specific tasks or domains. Basic Python programming skills are expected. Prior knowledge of standard LLM components (e.g. transformers, autoregression) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge.\n",
        "\n",
        "This tutorial combines detailed analysis and development of essential LLM Finetuning concepts via a practical exercise. Other necessary components will be developed using PyTorch. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss6wRHfE9KvI"
      },
      "source": [
        "# Notation\n",
        "Sections marked with [📚] contain cells that you should read, modify and complete to understand how your changes alter the obtained results.\n",
        "External resources are mentioned with [✨]. These provide valuable supplementary information for this tutorial and offer opportunities for further in-depth exploration of the topics covered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQebNMUR9R2D"
      },
      "source": [
        "# Libraries\n",
        "This tutorial leverages PyTorch for transformer implementation and training, complemented by standard Python libraries for data processing and the Hugging Face datasets and transformer library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95086qAd-wyb"
      },
      "source": [
        "# Hardware\n",
        "\n",
        "GPU access is recommended for optimal performance, particularly for model training and text generation. While all code can run on CPU, a CUDA-enabled environment will significantly speed up these processes.\n",
        "\n",
        "To grab a GPU (if available), make sure you go to Edit -> Notebook settings and choose a GPU under Hardware accelerator. Prefer **A100s** for higher flops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBEx0W5O_10p"
      },
      "source": [
        "# Credits\n",
        "The tutorial is created by:\n",
        "\n",
        "[Pranali Yawalkar](https://www.linkedin.com/in/pranali-yawalkar/)\n",
        "\n",
        "It is inspired by and synthesizes various online resources, which are cited throughout for reference and further reading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtC7A68GGOsY"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "Verified account on http://huggingface.co"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk8QogFMoG9D"
      },
      "source": [
        "# Step 1. Hugging Face Auth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcrRdBu1EnXG"
      },
      "source": [
        "To complete this tutorial, we need access to Gemma models on huggingface.\n",
        "\n",
        "1. You first need to follow the setup instructions at [HuggingFace User Tokens](https://huggingface.co/docs/hub/en/security-tokens) to create a new user access token.\n",
        "\n",
        "  * Choose **finegrained** scope of the token\n",
        "  * Ensure that **Read access to contents of all public gated repos you can access** is enabled under Profile -> Edit Access Token Permissions\n",
        "![](https://drive.google.com/uc?export=view&id=18JamIbfBMfd9bJ3uuetso1l12nmEsB4C)\n",
        "\n",
        "  * Copy and paste the token to colab secrets with key **HF_TOKEN**\n",
        "\n",
        "2. Get access to Google Gemma 2B model on [huggingface.co](https://huggingface.co/google/gemma-2b).\n",
        "3. When running the next cell, you'll be prompted with \"Grant access?\" messages - agree to provide colab secret access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnhQvsiAkBXt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFoO0kChoMRr"
      },
      "source": [
        "# Step 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA74zpbIkcCL"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q -U bitsandbytes==0.42.0\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers==4.38.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb3fFdSgoOxH"
      },
      "source": [
        "# 📚 Step 3. Load Gemma Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwhJc8LxJ5JL"
      },
      "source": [
        "## [Optional] Model Architecture\n",
        "\n",
        "For this tutorial, we'll be working with **Google Gemma 2B pretrained** model.\n",
        "\n",
        "\n",
        "\n",
        "1.   Decoder only model\n",
        "2.   Trained on context length of 8192 tokens ~ 6144 words (using the rule of thumb of 100 tokens ~= 75 words) at a time\n",
        "3.   More details on the non linear activation function, differences with the 7B model and explanations of various model dimensions at [Google Developers Site](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/)\n",
        "4.   We trade-off quality for lower computational cost and time, by choosing a 2B model for this exercise.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1B2L0-J9qJRuCLWcso3yIbR9-dpjbvj7Q)   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d6589Dfkfwu"
      },
      "outputs": [],
      "source": [
        "# @title 📚  Load Model\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GemmaTokenizer, GenerationConfig\n",
        "\n",
        "\n",
        "MODEL_ID = 'google/gemma-2b'\n",
        "\n",
        "def reload_model():\n",
        "  if 'MODEL' in globals():\n",
        "    del globals()['MODEL']\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      MODEL_ID,\n",
        "      quantization_config=None,\n",
        "      device_map={'': 0},\n",
        "      token=os.environ['HF_TOKEN'],\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "global TOKENIZER, MODEL\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID,\n",
        "                                          token=os.environ['HF_TOKEN'])\n",
        "MODEL = reload_model()\n",
        "\n",
        "\n",
        "def query_model(query: str) -> str:\n",
        "  # EXERCISE 1. Read PyTorch based documentation at\n",
        "  # ✨ https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "  # and complete this function\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmVPARFIiimI"
      },
      "outputs": [],
      "source": [
        "# @title Sample Query\n",
        "\n",
        "def sample_query(\n",
        "    query: str = (\n",
        "        \"Janet's ducks lay 16 eggs per day. She eats three for \"\n",
        "        \"breakfast every morning and bakes muffins for her friends every day \"\n",
        "        \"with four. She sells the remainder at the farmers' market daily for \"\n",
        "        \"$2 per fresh duck egg. How much in dollars does she make every day at \"\n",
        "        \"the farmers' market?\")):\n",
        "  print('Query: ', query)\n",
        "  print('Response: ', query_model(query))\n",
        "\n",
        "sample_query()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4jsyPm6PGyS"
      },
      "source": [
        "## Trainable Params\n",
        "\n",
        "These parameters are learnable parameters and are updated during training such that the predictive power of the model increases.\n",
        "\n",
        "\n",
        "1.   **Weights**: Measure the importance of each input or feature in predicting\n",
        "\n",
        "2.   **Biases**: A constant term added to the sum of weighted inputs\n",
        "the output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9R-7iqPC4Zu"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "  \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} || all params: {all_param} ||\"\n",
        "      f\" trainable: {100 * trainable_params / all_param:.2f}%\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLMY5mPMgz9i"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbekjdIgVbNX"
      },
      "source": [
        "# Step 4. LLM Training Fundamentals\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8m6VfCGbBsZ"
      },
      "source": [
        "## Finetuning Introduction\n",
        "\n",
        "In the world of large language models (LLMs), finetuning takes a pre-trained model (already trained on a massive amount of text data) and adapts it to perform better on a specific task, like:\n",
        "\n",
        "* Translating languages\n",
        "* Writing different kinds of creative content\n",
        "* Answering your questions in a more informative way\n",
        "\n",
        "Essentially, you're taking a general-purpose LLM and making it an expert in a particular area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay2ETqMZV58X"
      },
      "source": [
        "## Finetuning vs Pretraining key differences:\n",
        "\n",
        "# TODO(pranalipy) -- probe the audience here before showing the text\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1u_ha15E0oNwmb1BPFLNtySls9qxBaJU7)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyOLR45PeCor"
      },
      "source": [
        "## Finetuning Training Workflow\n",
        "\n",
        "A typical training workflow looks like:\n",
        "\n",
        "1. Identify the task you want to make the LLM better at 🧑\n",
        "2. Create evaluation data and framework that we want the LLM to get better at ❗\n",
        "3. Pick/ create datasets aka ground truth that truly represent the task 📚. Run:\n",
        "\n",
        "        *   Optional padding\n",
        "        *   Optional truncation\n",
        "        *   Shuffling\n",
        "        *   Batching\n",
        "        *   Repeating\n",
        "\n",
        "4. Training loop with various hyperparameters ➰:\n",
        "        *   Num epochs\n",
        "        *   Batch size\n",
        "        *   Max steps\n",
        "        *   Learning Rate\n",
        "        *   Optimizer -- rarely changed\n",
        "        *   Warm up steps\n",
        "5. Validation set for picking the best checkpoint ✅\n",
        "6. Rerun evaluations ❗\n",
        "7. Save the new checkpoint 💾\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCJROlxoobY"
      },
      "source": [
        "# 📚 Step 5. Load GSM8k data for Training and Evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uSvy-WorDng"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# EXERCISE 2: load the `gsm8k` dataset of `main` revision using `load_dataset`\n",
        "# ✨ Documentation at https://huggingface.co/docs/datasets/en/loading\n",
        "\n",
        "gsm8k = ...\n",
        "\n",
        "# EXERCISE 3: Get the `train` and `test` splits of this dataset and print\n",
        "# their lengths\n",
        "\n",
        "gsm8k_train, gsm8k_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsNeAZkuqwpY"
      },
      "outputs": [],
      "source": [
        "def cleanup_answer(example):\n",
        "  splits = example['answer'].split('####')\n",
        "  example['answer'] = splits[0].strip() + '\\nThe answer is ' + splits[1].strip()\n",
        "  return example\n",
        "\n",
        "print('Lenghts before cleanup, ', len(gsm8k_train), len(gsm8k_test))\n",
        "\n",
        "# EXERCISE 4: Map all entries of gsm8k_train and gsm8k_test with the\n",
        "# cleanup_answer function\n",
        "gsm8k_train = ...\n",
        "gsm8k_test = ...\n",
        "\n",
        "print('Lenghts after cleanup, ', len(gsm8k_train), len(gsm8k_test))\n",
        "\n",
        "# EXERCISE 5: Sample 10 entries from gsm8k_test to create a small eval set\n",
        "small_gsm8k_test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHcOiGAvUNU9"
      },
      "outputs": [],
      "source": [
        "def formatting_func(dataset: Dataset) -> list[str]:\n",
        "  ###\n",
        "  # EXERCISE 6: Format entries of example to follow a pattern like\n",
        "  #\n",
        "  # \"Question: ...\"\n",
        "  # \"Answer: ...\"\n",
        "  # EOS\n",
        "  #\n",
        "  # ✨ Documentation: https://huggingface.co/docs/trl/en/sft_trainer\n",
        "  # ✨ EOS documentation: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n",
        "  # As a fun exercise, try finetuning without EOS!\n",
        "  ###\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfJaFOGOC2rO"
      },
      "outputs": [],
      "source": [
        "print(gsm8k_train[0])\n",
        "print(gsm8k_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtKqOrjZAFg_"
      },
      "source": [
        "# Step 6. Prepare Evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQYrA8zRs5pW"
      },
      "outputs": [],
      "source": [
        "# @title Sample Eval\n",
        "sample_query()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csV50l2irK48"
      },
      "outputs": [],
      "source": [
        "# @title Bulk Evals\n",
        "\n",
        "\n",
        "# Sometimes the model outputs may not adhere to the provided format.\n",
        "def run_bulk_evals(\n",
        "    eval_set: Dataset,\n",
        "    prompt_prefix: str = '',\n",
        "    prompt_suffix: str = '',\n",
        "    answer_pattern: str = 'The answer is',\n",
        "    verbose: bool = False,\n",
        "):\n",
        "  correct = 0\n",
        "  for i in range(len(eval_set)):\n",
        "    problem = eval_set[i]\n",
        "    eval_prompt = prompt_prefix + problem['question'] + prompt_suffix\n",
        "    response = query_model(eval_prompt)\n",
        "\n",
        "    expected_response = problem['answer']\n",
        "    if answer_pattern in response:\n",
        "      lines = response.splitlines()\n",
        "      for l in reversed(lines):\n",
        "        if answer_pattern in l:\n",
        "          actual_response = l.split(answer_pattern)[1].strip()\n",
        "          actual_response = (\n",
        "              actual_response[:-1]\n",
        "              if actual_response.endswith('.')\n",
        "              else actual_response\n",
        "          )\n",
        "          parsed_expected_response = expected_response.split(answer_pattern)[\n",
        "              1\n",
        "          ].strip()\n",
        "          print(\n",
        "              'Parsed responses: ',\n",
        "              actual_response,\n",
        "              ' and ',\n",
        "              parsed_expected_response,\n",
        "          )\n",
        "          if actual_response == parsed_expected_response:\n",
        "            correct += 1\n",
        "          break\n",
        "\n",
        "    if verbose:\n",
        "      print(\n",
        "          '\\nExpected response: \\n',\n",
        "          expected_response,\n",
        "          ' \\n\\nFound response: ',\n",
        "          response,\n",
        "          '\\n********************************\\n',\n",
        "      )\n",
        "  return correct / len(eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMDCFWOFVbhd"
      },
      "outputs": [],
      "source": [
        "accuracy = run_bulk_evals(\n",
        "    small_gsm8k_test,\n",
        "    prompt_prefix='Answer the following question.\\n',\n",
        "    prompt_suffix=(\n",
        "        '\\nYou **must** end your answer with a line \"The answer is \" which'\n",
        "        ' contains the numerical answer such that it is easy for me to parse'\n",
        "        ' the answer. \\n'\n",
        "    ),\n",
        "    verbose=True\n",
        ")\n",
        "print('Accuracy: ', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo9T7eYgbDzM"
      },
      "source": [
        "# 📚 Step 6. Prepare Train Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcnD0NOHbNn3"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "def train(\n",
        "    *,\n",
        "    model: transformers.PreTrainedModel = MODEL,\n",
        "    num_epochs: int = 50,   # Note: Why did we set it so high?\n",
        "    learning_rate: float = 2e-5,\n",
        "    dataset: Dataset = gsm8k_train,\n",
        "    lora_config: LoraConfig = None,\n",
        "):\n",
        "  # EXERCISE 6: Create TrainingArguments by reading\n",
        "  # ✨ https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/trainer#transformers.TrainingArguments\n",
        "  # with the above epochs, learning rate, ✨`paged_adamw_8bit` optimizer\n",
        "  # and bf16 mode.\n",
        "\n",
        "  # EXERCISE 7: What's the best strategy to set the batch size?\n",
        "  args = transformers.TrainingArguments()\n",
        "\n",
        "  # Updates model in place\n",
        "  trainer = SFTTrainer(\n",
        "      model=model,\n",
        "      train_dataset=dataset,\n",
        "      args=args,\n",
        "      peft_config=lora_config,\n",
        "      formatting_func=formatting_func,\n",
        "  )\n",
        "  trainer.train()\n",
        "  return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FB0IwIPWENy"
      },
      "source": [
        "# 📚 Step 8. Finetune!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_u9vsRZWzgi"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "✨ https://huggingface.co/docs/trl/en/sft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-emcbeGW6P0"
      },
      "source": [
        "## 📚  Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPH4Y41QWGe5"
      },
      "outputs": [],
      "source": [
        "MODEL = reload_model()\n",
        "trainer = train(model=MODEL)\n",
        "\n",
        "# EXERCISE 8: Identify what batch size was used for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCj7Vd-HdLyX"
      },
      "outputs": [],
      "source": [
        "# EXERCISE 9: explore learning rates in the set\n",
        "# [5e-2, 5e-3, 2e-4, 5e-4, 2e-5, 5e-5] to find the best\n",
        "# one for our setup\n",
        "\n",
        "# Usually we use validation set loss for identifying the best LR.\n",
        "# For simplicity, we'll use training loss as a proxy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4pH92fW9YZ"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRBWVTjVWLhH"
      },
      "outputs": [],
      "source": [
        "# @title Sample Eval Again\n",
        "\n",
        "query_model(\n",
        "    'Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast'\n",
        "    ' every morning and bakes muffins for her friends every day with four. She'\n",
        "    \" sells the remainder at the farmers' market daily for $2 per fresh duck\"\n",
        "    \" egg. How much in dollars does she make every day at the farmers' market?\"\n",
        "    ' \\nAnswer: '\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUFLyZQbWNbL"
      },
      "outputs": [],
      "source": [
        "# @title Bulk Evals Again\n",
        "accuracy = run_bulk_evals(\n",
        "    small_gsm8k_test,\n",
        "    prompt_prefix='Question: ',\n",
        "    prompt_suffix=' \\nAnswer: ',\n",
        "    answer_pattern='The answer is',\n",
        "    verbose=True\n",
        ")\n",
        "print('Accuracy: ', accuracy * 100, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx7EjGabrMp3"
      },
      "source": [
        "# 📚  Step 9. LoRA Finetune!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf6JZMDoOPQb"
      },
      "source": [
        "## LoRA Concepts\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1534a4LeyfTSegnm4Dnfxx6_mWB0BMUOZ)\n",
        "\n",
        "\n",
        "1. **Efficient Parameter Adjustment**: Instead of modifying all the original model's parameters (which are numerous and resource-intensive), LoRA introduces trainable rank decomposition matrices into each layer of the Transformer architecture. These matrices have a much smaller size than the original model's weight matrices.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Kz68zGJ_9IuZWQ-0tcGaD-Azm5G4GDCW)\n",
        "\n",
        "\n",
        "2. **Reduced Memory and Comppute Footprint**: By training only these smaller matrices, LoRA significantly reduces the memory requirements and computational costs associated with fine-tuning large models.\n",
        "\n",
        "\n",
        "3. **Avoids Catastrophic Forgetting**: The original model's weights remain frozen, and the LoRA matrices capture the task-specific knowledge, allowing the model to adapt to new data or tasks without altering its core capabilities.\n",
        "\n",
        "\n",
        "4. **Merging with the Original Model**: During inference, the low-rank matrices are merged back with the original weights, effectively injecting the learned knowledge into the model.\n",
        "\n",
        "\n",
        "5. **Modularity and Reusability**: The trained LoRA modules can be easily shared, reused, and combined with different pre-trained models, enabling flexible and efficient adaptation across various tasks.\n",
        "\n",
        "\n",
        "**LoRA has been shown to achieve comparable or even better performance than full fine-tuning, while using significantly fewer resources.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxtKOQeYXCVM"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "*   ✨ https://huggingface.co/docs/trl/en/sft_trainer\n",
        "*   ✨ https://huggingface.co/docs/peft/en/package_reference/lora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocW5DpAtgcDA"
      },
      "source": [
        "## LoRA Hypers\n",
        "\n",
        "✨ [LoRA Hypers doc](https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1eGxjTrnkpHxM8VN_SKJLa6yff0gmxJYP)\n",
        "\n",
        "\n",
        "\n",
        "1.   **r**: rank of the LoRA matrices that capture information about our task. Higher the better. Typically, **low ranks: 8, 16, 32** and **high ranks: 128, 256**\n",
        "2.   **lora_alpha**: Alpha is a scaling factor -- it changes how the LoRA's weights are weighed against the base model's. Higher alpha means the LoRA layers act more strongly than the base model. Typically set to **r** or **2 x r**. The original LoRA method uses the scalar function **lora_alpha/r** for scaling LoRA weights during forward pass.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sahILdJsICvB"
      },
      "source": [
        "##  📚 Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n_0dySxoobZ"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "base_lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"o_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fom7ZZDmooba"
      },
      "outputs": [],
      "source": [
        "# EXERCISE 10: Hyper r from [32, 64, 128] and lora_alpha from [r, 2*r]\n",
        "MODEL = reload_model()\n",
        "train(model=MODEL, lora_config=lora_config)\n",
        "print(print_trainable_parameters(MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfhMvR9HIFSs"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJXsTAygZS2y"
      },
      "outputs": [],
      "source": [
        "# @title Sample Eval Again\n",
        "query_model(\n",
        "    'Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast'\n",
        "    ' every morning and bakes muffins for her friends every day with four. She'\n",
        "    \" sells the remainder at the farmers' market daily for $2 per fresh duck\"\n",
        "    \" egg. How much in dollars does she make every day at the farmers' market?\"\n",
        "    ' \\nAnswer: '\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr4u56kuVf6K"
      },
      "outputs": [],
      "source": [
        "# @title Bulk Evals Again\n",
        "accuracy = run_bulk_evals(\n",
        "    small_gsm8k_test,\n",
        "    prompt_prefix='Question: ',\n",
        "    prompt_suffix=' \\nAnswer: ',\n",
        "    answer_pattern='The answer is',\n",
        "    verbose=True,\n",
        ")\n",
        "print('Accuracy: ', accuracy * 100, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM0UjlUAE_Nk"
      },
      "source": [
        "# 📚  [Advanced] Step 10. Custom PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ivmThKOxme"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "\n",
        "\n",
        "1.   [SFTTrainer](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py)\n",
        "2.   [PEFT](https://github.com/huggingface/peft)\n",
        "3.   [Transformers](https://github.com/huggingface/transformers)\n",
        "4. [PyTorch NN](https://pytorch.org/docs/stable/nn.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSlTBp69PRQP"
      },
      "source": [
        "## 📚  Create Custom LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dczRnFiOFBkL"
      },
      "outputs": [],
      "source": [
        "from peft.tuners.lora.layer import Linear\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CustomLinearLoRA(Linear):\n",
        "\n",
        "  def update_layer(\n",
        "      self,\n",
        "      adapter_name,\n",
        "      r,\n",
        "      lora_alpha,\n",
        "      lora_dropout,\n",
        "      init_lora_weights,\n",
        "      use_rslora=False,\n",
        "      use_dora=False,\n",
        "  ):\n",
        "    # This code works for linear layers, override for other layer types\n",
        "    if r <= 0:\n",
        "      raise ValueError(\n",
        "          f\"`r` should be a positive integer value but the value passed is {r}\"\n",
        "      )\n",
        "\n",
        "    self.r[adapter_name] = r\n",
        "    self.lora_alpha[adapter_name] = lora_alpha\n",
        "\n",
        "    # EXERCISE 11: Define a dropout layer\n",
        "    lora_dropout_layer = ...\n",
        "\n",
        "    self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n",
        "\n",
        "    # Actual trainable parameters\n",
        "    # EXERCISE 12: write a linear layer that goes from self.in_features to r\n",
        "    self.lora_A[adapter_name] = ...\n",
        "    # EXERCISE 13: write a linear layer that goes from r to self.out_features\n",
        "    self.lora_B[adapter_name] = ...\n",
        "\n",
        "    self.scaling[adapter_name] = lora_alpha / r\n",
        "\n",
        "    self.reset_lora_parameters(adapter_name, init_lora_weights)\n",
        "    self.set_adapter(self.active_adapters)\n",
        "\n",
        "  def forward(self, x, *args, **kwargs):\n",
        "    result = self.base_layer(x, *args, **kwargs)\n",
        "    torch_result_dtype = result.dtype\n",
        "    for active_adapter in self.active_adapters:\n",
        "      if active_adapter not in self.lora_A.keys():\n",
        "        continue\n",
        "      lora_A = self.lora_A[active_adapter]\n",
        "      lora_B = self.lora_B[active_adapter]\n",
        "      dropout = self.lora_dropout[active_adapter]\n",
        "      scaling = self.scaling[active_adapter]\n",
        "\n",
        "      x = x.to(lora_A.weight.dtype)\n",
        "\n",
        "      x = dropout(x)\n",
        "\n",
        "      # EXERCISE 14: add to the result of the base layer, the output of\n",
        "      # lora_B and lora_A and multiply by the scaling factor\n",
        "      result = ...\n",
        "\n",
        "    result = result.to(torch_result_dtype)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiXUQT4EHor6"
      },
      "source": [
        "## Orchestrating our PeFT Model with Huggingface\n",
        "\n",
        "Since we are using the HuggingFace PEFT library framework, we need to tweak some of its internal workings to be able to expose the LoRA layer above. Therefore the following cell is not very insightful to understand during this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTGy0A06FU2z"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, LoraModel, PeftModel, get_peft_model\n",
        "from peft.tuners.lora.layer import Linear, dispatch_default\n",
        "from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers.utils import PushToHubMixin\n",
        "\n",
        "\n",
        "def custom_dispatch_default(\n",
        "    target: torch.nn.Module, adapter_name, lora_config, **kwargs\n",
        "):\n",
        "  new_module = None\n",
        "  target_base_layer = (\n",
        "      target.get_base_layer() if isinstance(target, BaseTunerLayer) else target\n",
        "  )\n",
        "\n",
        "  if isinstance(target_base_layer, torch.nn.Linear):\n",
        "    kwargs.update(lora_config.loftq_config)\n",
        "    new_module = CustomLinearLoRA(target, adapter_name, **kwargs)\n",
        "\n",
        "  if new_module is None:\n",
        "    new_module = dispatch_default(\n",
        "        target, adapter_name, lora_config=lora_config, **kwargs\n",
        "    )\n",
        "  return new_module\n",
        "\n",
        "\n",
        "class CustomLoraModel(LoraModel):\n",
        "\n",
        "  @staticmethod\n",
        "  def _create_new_module(lora_config, adapter_name, target, **kwargs):\n",
        "    return custom_dispatch_default(\n",
        "        target, adapter_name, lora_config=lora_config, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "class CustomPeftModel(PeftModel):\n",
        "\n",
        "  def __init__(self, model, peft_config, adapter_name=\"default\"):\n",
        "    PushToHubMixin.__init__(self)\n",
        "    torch.nn.Module.__init__(self)\n",
        "\n",
        "    self.modules_to_save = None\n",
        "    self.active_adapter = adapter_name\n",
        "    self.peft_type = peft_config.peft_type\n",
        "    # These args are special PEFT arguments that users can pass.\n",
        "    # They need to be removed before passing them to forward.\n",
        "    self.special_peft_forward_args = {\"adapter_names\"}\n",
        "\n",
        "    self._is_prompt_learning = peft_config.is_prompt_learning\n",
        "    self._peft_config = None\n",
        "    self.base_model = CustomLoraModel(\n",
        "        model, {adapter_name: peft_config}, adapter_name\n",
        "    )\n",
        "\n",
        "    self.set_additional_trainable_modules(peft_config, adapter_name)\n",
        "\n",
        "    if getattr(model, \"is_gradient_checkpointing\", True):\n",
        "      model = self._prepare_model_for_gradient_checkpointing(model)\n",
        "\n",
        "    # the `pretraining_tp` is set for some models to simulate Tensor\n",
        "    # Parallelism during inference to avoid numerical differences,\n",
        "    # ✨ https://github.com/pytorch/pytorch/issues/76232 - to avoid any unexpected\n",
        "    # behavior we disable that in this line.\n",
        "    if hasattr(self.base_model, \"config\") and hasattr(\n",
        "        self.base_model.config, \"pretraining_tp\"\n",
        "    ):\n",
        "      self.base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bCasYVRHwKY"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxQKk5fbFZ2Z"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"o_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "MODEL = reload_model()\n",
        "MODEL = CustomPeftModel(MODEL, lora_config)\n",
        "trainer = train(model=MODEL, lora_config=lora_config, num_epochs=50)\n",
        "print(print_trainable_parameters(MODEL))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8IfM-uNIX1z"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lGDPVJ7IdfE"
      },
      "outputs": [],
      "source": [
        "# @title Sample Eval\n",
        "query_model(\n",
        "    'Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast'\n",
        "    ' every morning and bakes muffins for her friends every day with four. She'\n",
        "    \" sells the remainder at the farmers' market daily for $2 per fresh duck\"\n",
        "    \" egg. How much in dollars does she make every day at the farmers' market?\"\n",
        "    ' \\nAnswer: '\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D4jyDXKGKex"
      },
      "outputs": [],
      "source": [
        "# @title Bulk Evals Again\n",
        "accuracy = run_bulk_evals(\n",
        "    small_gsm8k_test,\n",
        "    prompt_prefix='Question: ',\n",
        "    prompt_suffix=' \\nAnswer: ',\n",
        "    answer_pattern='The answer is',\n",
        "    verbose=True,\n",
        ")\n",
        "print('Accuracy: ', accuracy * 100, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBI0f4lYdQ5R"
      },
      "source": [
        "# Other PEFT Techniques\n",
        "\n",
        "\n",
        "1.   Quantization [doc](https://huggingface.co/docs/peft/en/developer_guides/quantization)\n",
        "2.   Quantization + LoRA [doc](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
        "3.   Soft prompt tuning [doc](https://huggingface.co/docs/peft/en/conceptual_guides/prompting)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu98-_ZlGpQZ"
      },
      "source": [
        "# ✨ Resources used for this tutorial and references\n",
        "\n",
        "\n",
        "\n",
        "1. [Gemma Architecture](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/)  \n",
        "2. [Huggingface LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
        "3.   [Huggingface Finetuning](https://huggingface.co/docs/transformers/en/training)\n",
        "4. [Finetuning Gemma for Math](https://medium.com/google-developer-experts/fine-tuning-gemma-2b-to-solve-math-problems-ac4921ed531e)\n",
        "5. [Gemma PyTorch](https://github.com/google/gemma_pytorch)\n",
        "6. [Gemma Finetuning](https://github.com/google-deepmind/gemma/blob/main/colabs/fine_tuning_tutorial.ipynb)\n",
        "7. [LLM Optimizers](https://towardsdatascience.com/fine-tuning-llms-with-32-bit-8-bit-and-paged-adamw-optimizers-1034e3105634)\n",
        "8. [LoRA Hypers](https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133)\n",
        "9. [Simple Gemma PEFT](https://huggingface.co/blog/gemma-peft)\n",
        "10. [M2Lschool Previous Tutorial](https://github.com/M2Lschool/tutorials2024/blob/main/1_nlp/part_III_llm_finetuning/LoRA.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}