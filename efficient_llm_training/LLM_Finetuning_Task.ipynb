{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LufV608o36pC"
      },
      "source": [
        "# Efficient LLM Training Tutorial\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MENA-ML/tutorials2025-tasks/blob/main/efficient_llm_training/LLM_Finetuning_Task.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This is the tutorial for the **2025 Middle East and North Africa Machine Learning (MenaML) Winter School**!\n",
        "\n",
        "This tutorial will explore the fundamental aspects of Finetuning Large Language Models for specific tasks or domains. Basic Python programming skills are expected. Prior knowledge of standard LLM components (e.g. transformers, autoregression) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge.\n",
        "\n",
        "This tutorial combines detailed analysis and development of essential LLM Finetuning concepts via a practical exercise. Other necessary components will be developed using PyTorch. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss6wRHfE9KvI"
      },
      "source": [
        "# Notation\n",
        "Sections marked with [ðŸ“š] contain cells that you should read, modify and complete to understand how your changes alter the obtained results.\n",
        "External resources are mentioned with [âœ¨]. These provide valuable supplementary information for this tutorial and offer opportunities for further in-depth exploration of the topics covered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQebNMUR9R2D"
      },
      "source": [
        "# Libraries\n",
        "This tutorial leverages PyTorch for transformer implementation and training, complemented by standard Python libraries for data processing and the Hugging Face datasets and transformer library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95086qAd-wyb"
      },
      "source": [
        "# Hardware\n",
        "\n",
        "GPU access is recommended for optimal performance, particularly for model training and text generation. CUDA-enabled environment will significantly speed up these processes. Please connect to the free colab runtime with **T4 GPU** runtime type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBEx0W5O_10p"
      },
      "source": [
        "# Credits\n",
        "The tutorial is created by:\n",
        "\n",
        "[Pranali Yawalkar](https://www.linkedin.com/in/pranali-yawalkar/)\n",
        "\n",
        "It is inspired by and synthesizes various online resources, which are cited throughout for reference and further reading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtC7A68GGOsY"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "Verified account on http://huggingface.co"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk8QogFMoG9D"
      },
      "source": [
        "# Step 1. Hugging Face Auth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcrRdBu1EnXG"
      },
      "source": [
        "\n",
        "1. You first need to follow the setup instructions at [HuggingFace User Tokens](https://huggingface.co/docs/hub/en/security-tokens) to create a new user access token.\n",
        "\n",
        "  * Choose **finegrained** scope of the token\n",
        "  * Ensure that **Read access to contents of all public gated repos you can access** is enabled under Profile -> Edit Access Token Permissions\n",
        "![](https://drive.google.com/uc?export=view&id=18JamIbfBMfd9bJ3uuetso1l12nmEsB4C)\n",
        "\n",
        "  * Copy and paste the token to colab secrets with key **HF_TOKEN**\n",
        "\n",
        "2. When running the next cell, you'll be prompted with \"Grant access?\" messages - agree to provide colab secret access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnhQvsiAkBXt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFoO0kChoMRr"
      },
      "source": [
        "# Step 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA74zpbIkcCL"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q -U bitsandbytes==0.45.2\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers==4.45.2\n",
        "!pip3 install -q -U trl==0.11.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb3fFdSgoOxH"
      },
      "source": [
        "# ðŸ“š Step 3. Load GPT-2 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwhJc8LxJ5JL"
      },
      "source": [
        "## [Optional] Model Architecture\n",
        "\n",
        "For this tutorial, we'll be working with **GPT-2 pretrained** model.\n",
        "\n",
        "\n",
        "\n",
        "1.   Decoder only model\n",
        "2.   Trained on context length of 1024 tokens\n",
        "3.   This is the smallest version of GPT-2, with **124M** parameters  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d6589Dfkfwu"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ“š  Load Model\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "def reload_model():\n",
        "  m = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(\"cuda:0\")\n",
        "  return m\n",
        "\n",
        "\n",
        "global TOKENIZER, MODEL\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "if TOKENIZER.pad_token is None:\n",
        "  TOKENIZER.pad_token = TOKENIZER.eos_token\n",
        "MODEL = reload_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inferences\n",
        "\n",
        "\n",
        "def query_model(model, query: str, max_tokens:int = 64) -> str:\n",
        "  # EXERCISE 1. Read PyTorch based documentation at\n",
        "  # https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "  # and complete this function for generation.\n",
        "\n",
        "  # Please add `pad_token_id=TOKENIZER.eos_token_id` to the generate args\n",
        "  # for masking attention on padded tokens.\n",
        "  ..."
      ],
      "metadata": {
        "id": "nJdPfXcQKcjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmVPARFIiimI"
      },
      "outputs": [],
      "source": [
        "# @title Sample Query\n",
        "\n",
        "\n",
        "def sample_query(\n",
        "    query: str = (\n",
        "        'Give me a bulleted list of the 5 highest mountains in the world and'\n",
        "        ' their respective heights in meters'\n",
        "    ),\n",
        "):\n",
        "  print('Query: ', query)\n",
        "  print('Response: ', query_model(MODEL, query))\n",
        "\n",
        "\n",
        "sample_query()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbekjdIgVbNX"
      },
      "source": [
        "# Step 4. LLM Training Fundamentals\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8m6VfCGbBsZ"
      },
      "source": [
        "## Finetuning Introduction\n",
        "\n",
        "We will be finetuning the GPT-2 model with Dolly 15k dataset contains 15,000 high-quality human-generated prompt / response pairs specifically designed for instruction tuning LLMs. The data cover brainstorming, classification, closed QA, generation, information extraction, open QA and summarization type of tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyOLR45PeCor"
      },
      "source": [
        "## Finetuning Training Workflow\n",
        "\n",
        "A typical training workflow looks like:\n",
        "\n",
        "1. Identify the task you want to make the LLM better at ðŸ§‘\n",
        "2. Create evaluation data and framework that we want the LLM to get better at â—\n",
        "3. Pick/ create datasets aka ground truth that truly represent the task ðŸ“š\n",
        "4. Data mixture recipes ðŸ“š:\n",
        "        *   Packing\n",
        "        *   Optional padding\n",
        "        *   Optional truncation\n",
        "        *   Shuffling\n",
        "        *   Batching\n",
        "        *   Repeating\n",
        "        *   Weighted mixtures\n",
        "\n",
        "5. Training loop with various hyperparameters âž°:\n",
        "        *   Num epochs\n",
        "        *   Batch size\n",
        "        *   Learning Rate\n",
        "        *   Optimizer -- rarely changed\n",
        "        *   Warm up steps, learning schedule\n",
        "6. Validation set for picking the best checkpoint âœ…\n",
        "7. Rerun evaluations â—\n",
        "8. Save the new checkpoint ðŸ’¾\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCJROlxoobY"
      },
      "source": [
        "# ðŸ“š Step 5. Load Dolly15k data for Training and Evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uSvy-WorDng"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# EXERCISE 2: load the `databricks-dolly-15k` dataset of `main` revision using\n",
        "# `load_dataset`\n",
        "# âœ¨ Documentation at https://huggingface.co/docs/datasets/en/loading\n",
        "\n",
        "ds = load_dataset(\"databricks/databricks-dolly-15k\", \"default\",\n",
        "                  cache_dir='/tmp', keep_in_memory=True)\n",
        "\n",
        "# EXERCISE 3: Create the `train` and `test` splits of this dataset and print\n",
        "# their lengths.\n",
        "# Split 14000 entries into `ds_train`, 1k into `ds_test`, and sample 10 from the\n",
        "# `test` into another smaller set `small_ds_test` for eyeballing.\n",
        "\n",
        "ds_train = ds['train'].select(range(14000))\n",
        "ds_test = ds['train'].select(range(14000, len(ds['train'])))\n",
        "small_ds_test = ds_test.select(range(10))\n",
        "print(len(ds_train), len(ds_test), len(small_ds_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHcOiGAvUNU9"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# EXERCISE 4: Format entries of train example to follow a pattern like\n",
        "#\n",
        "# \"Query: ...\"\n",
        "# \"Response: ...\"\n",
        "# EOS\n",
        "#\n",
        "# âœ¨ Documentation: https://huggingface.co/docs/trl/en/sft_trainer\n",
        "# âœ¨ EOS documentation: https://www.natebrake.com/blog/llm/end-of-sequence-explained\n",
        "# As a fun exercise, try finetuning without EOS!\n",
        "###\n",
        "\n",
        "\n",
        "def train_formatting_func(element) -> dict[str]:\n",
        "  # Return {\"text\": formatted_element}. We use the \"text\" key while training.\n",
        "  ...\n",
        "\n",
        "\n",
        "###\n",
        "# EXERCISE 5: Format entries of test example to follow a pattern like\n",
        "#\n",
        "# \"Query: ...\"\n",
        "# \"Response:\"\n",
        "###\n",
        "def test_formatting_func(element) -> dict[str]:\n",
        "  # Return {\"text\": formatted_element}. We use the \"text\" key while evaluating.\n",
        "  ...\n",
        "\n",
        "print(ds_train[0])\n",
        "print(small_ds_test[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“š Step 6. Evals"
      ],
      "metadata": {
        "id": "rlADI9O3LNbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(small_ds_test)):\n",
        "  query = small_ds_test['instruction'][i]\n",
        "  predicted_output = query_model(MODEL, query, max_tokens=64)\n",
        "  actual_output = small_ds_test['response'][i]\n",
        "  print(i)\n",
        "  print('True response')\n",
        "  print(actual_output)\n",
        "  print('--------------')\n",
        "  print('Predicted response')\n",
        "  print(predicted_output)\n",
        "  print(\"****************\")"
      ],
      "metadata": {
        "id": "J7W8LLTXLH0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR6nfYR7q3Lm"
      },
      "source": [
        "# ðŸ“š Step 7. Prepare Train Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Loss Curves (Purely orchestration code)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "\n",
        "class LossCallback(TrainerCallback):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "  ):\n",
        "    self.loss_history = []\n",
        "\n",
        "  def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "    if state.is_local_process_zero and logs:\n",
        "      if 'loss' in logs:\n",
        "        self.loss_history.append(logs['loss'])\n",
        "\n",
        "\n",
        "# Function to plot the loss curve\n",
        "def plot_loss_curve(losses, smoothing=0.1):\n",
        "  # Create steps for x-axis\n",
        "  steps = np.arange(10, (len(losses) + 1) * 10, step=10)\n",
        "\n",
        "  # Calculate smoothed losses for better visualization\n",
        "  if smoothing > 0:\n",
        "    smoothed_losses = []\n",
        "    for i in range(len(losses)):\n",
        "      if i == 0:\n",
        "        smoothed_losses.append(losses[i])\n",
        "      else:\n",
        "        smoothed_loss = (\n",
        "            smoothing * losses[i] + (1 - smoothing) * smoothed_losses[-1]\n",
        "        )\n",
        "        smoothed_losses.append(smoothed_loss)\n",
        "  else:\n",
        "    smoothed_losses = losses\n",
        "\n",
        "  # Create the plot\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  plt.plot(steps, losses, label='Raw Loss', alpha=0.3)\n",
        "  # plt.plot(steps, smoothed_losses, label='Smoothed Loss', linewidth=2)\n",
        "\n",
        "  # Customize the plot\n",
        "  plt.xlabel('Training Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training Loss Curve')\n",
        "  plt.grid(True, linestyle='--', alpha=0.7)\n",
        "  plt.legend()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Za6hAy3zaJhv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jVfS7URq6PC"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
        "\n",
        "\n",
        "def train(\n",
        "    *,\n",
        "    learning_rate: float,\n",
        "    model: transformers.PreTrainedModel = MODEL,\n",
        "    num_epochs: int = 1,\n",
        "    dataset: Dataset = ds_train,\n",
        "    lora_config: LoraConfig = None,\n",
        "    max_steps: int = -1,\n",
        "):\n",
        "  # EXERCISE 6: Create SFTConfig by reading\n",
        "  # âœ¨ https://huggingface.co/docs/trl/v0.14.0/en/sft_trainer#trl.SFTConfig\n",
        "  # with the above epochs, learning rate. Other constant hypers are:\n",
        "  # âœ¨`paged_adamw_8bit` optimizer\n",
        "  # 5% warmup ratio\n",
        "  # \"cosine\" lr schedule\n",
        "  # fp16 True\n",
        "  # log loss every 10 steps\n",
        "  # dataset_text_field to 'text'\n",
        "\n",
        "  # Github if needed: https://github.com/huggingface/trl/blob/main/trl/trainer/sft_config.py\n",
        "  # https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\n",
        "\n",
        "  # EXERCISE 7: What's the best strategy to set the batch size?\n",
        "\n",
        "  # EXERCISE 8: What's the best strategy to set packing to?\n",
        "  args = SFTConfig(...)\n",
        "\n",
        "  # Updates model in place\n",
        "  trainer = SFTTrainer(\n",
        "      model=model,\n",
        "      train_dataset=dataset,\n",
        "      args=args,\n",
        "      peft_config=lora_config,\n",
        "  )\n",
        "  callback = LossCallback()\n",
        "  trainer.add_callback(callback)\n",
        "\n",
        "  trainer.train()\n",
        "  plot_loss_curve(callback.loss_history)\n",
        "  return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FB0IwIPWENy"
      },
      "source": [
        "# ðŸ“š  Step 8. Finetune!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_u9vsRZWzgi"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "âœ¨ https://huggingface.co/docs/trl/en/sft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-emcbeGW6P0"
      },
      "source": [
        "## ðŸ“š  Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHkAJhbnPX4s"
      },
      "outputs": [],
      "source": [
        "# @title Learning Rate Sweep\n",
        "\n",
        "# EXERCISE 10: sweep over learning rates in the set\n",
        "# [5e-2, 5e-3, 5e-4, 5e-5, 5e-6] to find the best\n",
        "# one that minimises the training loss and does not overfit. Consider\n",
        "# running for a small fixed number of steps like 50.\n",
        "# A good learning rate should allow:\n",
        "\n",
        "# Gradual loss reduction\n",
        "# Sufficient exploration of the loss landscape to avoid local minimas\n",
        "# Time for the model to learn meaningful patterns\n",
        "# Balance between convergence speed and generalization\n",
        "\n",
        "for lr in [5e-2, 5e-3, 5e-4, 5e-5, 5e-6]:\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPH4Y41QWGe5"
      },
      "outputs": [],
      "source": [
        "# @title Run Full Finetuning\n",
        "\n",
        "MODEL = reload_model()\n",
        "\n",
        "# EXERCISE 11: Run the full train run for 2 epochs using the learning_rate\n",
        "# chosen above\n",
        "\n",
        "trainer = ...\n",
        "\n",
        "# EXERCISE 12: Identify what batch size was used for training\n",
        "# from this https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4pH92fW9YZ"
      },
      "source": [
        "# Step 9. Run Evals for Finetuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRBWVTjVWLhH"
      },
      "outputs": [],
      "source": [
        "def run_ft_evals():\n",
        "  for i in range(len(small_ds_test)):\n",
        "    # EXERCISE 13: Rewrite the query and parse the output correctly to adhere to\n",
        "    # the formatting constraints from step 5.\n",
        "    formatted_query = small_ds_test['text'][i]\n",
        "    output = query_model(MODEL, formatted_query, max_tokens=64)\n",
        "    predicted_output = output.split(\"Response: \")[1]\n",
        "\n",
        "    actual_output = small_ds_test['response'][i]\n",
        "    print(i)\n",
        "    print('True response')\n",
        "    print(actual_output)\n",
        "    print('--------------')\n",
        "    print('Predicted response')\n",
        "    print(predicted_output)\n",
        "    print(\"****************\")\n",
        "\n",
        "\n",
        "run_ft_evals()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx7EjGabrMp3"
      },
      "source": [
        "# Step 10. LoRA Finetune!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4jsyPm6PGyS"
      },
      "source": [
        "## Identify Trainable Params\n",
        "\n",
        "These parameters are learnable parameters and are updated during training such that the predictive power of the model increases.\n",
        "\n",
        "\n",
        "1.   **Weights**: Measure the importance of each input or feature in predicting\n",
        "\n",
        "2.   **Biases**: A constant term added to the sum of weighted inputs\n",
        "the output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9R-7iqPC4Zu"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "  \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} || all params: {all_param} ||\"\n",
        "      f\" trainable: {100 * trainable_params / all_param:.2f}%\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLMY5mPMgz9i"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf6JZMDoOPQb"
      },
      "source": [
        "## LoRA Concepts\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1534a4LeyfTSegnm4Dnfxx6_mWB0BMUOZ)\n",
        "\n",
        "\n",
        "1. **Efficient Parameter Adjustment**: Instead of modifying all the original model's parameters (which are numerous and resource-intensive), LoRA introduces trainable rank decomposition matrices into each layer of the Transformer architecture. These matrices have a much smaller size than the original model's weight matrices.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Kz68zGJ_9IuZWQ-0tcGaD-Azm5G4GDCW)\n",
        "\n",
        "\n",
        "2. **Reduced Memory Footprint**: By training only these smaller matrices, LoRA significantly reduces the memory requirements associated with fine-tuning large models.\n",
        "\n",
        "\n",
        "3. **Avoids Catastrophic Forgetting**: The original model's weights remain frozen, and the LoRA matrices capture the task-specific knowledge, allowing the model to adapt to new data or tasks without altering its core capabilities.\n",
        "\n",
        "\n",
        "4. **Merging with the Original Model**: During inference, the low-rank matrices are merged back with the original weights, effectively injecting the learned knowledge into the model.\n",
        "\n",
        "\n",
        "5. **Modularity and Reusability**: The trained LoRA modules can be easily swapped, enabling flexible and efficient adaptation across various tasks.\n",
        "\n",
        "\n",
        "**LoRA has been shown to achieve comparable or even better performance than full fine-tuning, while using significantly fewer resources.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxtKOQeYXCVM"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "*   âœ¨ https://huggingface.co/docs/trl/en/sft_trainer\n",
        "*   âœ¨ https://huggingface.co/docs/peft/en/package_reference/lora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8bcRKv_sVFk"
      },
      "source": [
        "## LoRA Hypers\n",
        "\n",
        "âœ¨ [LoRA Hypers doc](https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1eGxjTrnkpHxM8VN_SKJLa6yff0gmxJYP)\n",
        "\n",
        "\n",
        "\n",
        "1.   **r**: rank of the LoRA matrices that capture information about our task. Higher the better. Typically, **low ranks: 8, 16, 32** and **high ranks: 128, 256**\n",
        "2.   **lora_alpha**: Alpha is a scaling factor -- it changes how the LoRA's weights are weighed against the base model's. Higher alpha means the LoRA layers act more strongly than the base model. Typically set to **r** or **2 x r**. The original LoRA method uses the scalar function **lora_alpha/r** for scaling LoRA weights during forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sahILdJsICvB"
      },
      "source": [
        "## ðŸ“š Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fom7ZZDmooba"
      },
      "outputs": [],
      "source": [
        "# @title LoRA Hyper Sweep\n",
        "\n",
        "# EXERCISE 14: sweep over r in the set [32, 64, 128] and lora_alpha over\n",
        "# [r, 2xr] to find the best values that give the least training loss.\n",
        "# Consider running for a small fixed number of steps like 50.\n",
        "\n",
        "# EXERCISE 15: For every sweep, also print the trainable params for the\n",
        "# finetuned model.\n",
        "\n",
        "# Set learning_rate to one order of magnitude higher than full finetuning\n",
        "# as LoRA typically needs a higher LR for the reduced set of trainable params.\n",
        "\n",
        "from peft import LoraConfig\n",
        "\n",
        "base_lora_config = LoraConfig(\n",
        "    lora_dropout=0.01,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM',\n",
        ")\n",
        "\n",
        "for r in [32, 64, 128]:\n",
        "  for lora_alpha in [r, 2 * r]:\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill these values with the chosen hypers\n",
        "LORA_LEARNING_RATE = ...\n",
        "LORA_R = ...\n",
        "LORA_ALPHA = ..."
      ],
      "metadata": {
        "id": "j7WtRb7lhGX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n_0dySxoobZ"
      },
      "outputs": [],
      "source": [
        "# @title Run LoRA.\n",
        "\n",
        "# EXERCISE 16: Run for 2 epochs using the hypers tuned above.\n",
        "# Remember to use LORA_LEARNING_RATE\n",
        "\n",
        "base_lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "MODEL = reload_model()\n",
        "trainer = ...\n",
        "print(print_trainable_parameters(MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfhMvR9HIFSs"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJXsTAygZS2y"
      },
      "outputs": [],
      "source": [
        "run_ft_evals()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM0UjlUAE_Nk"
      },
      "source": [
        "# ðŸ“š  [Advanced] Step 11. Custom PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ivmThKOxme"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "\n",
        "\n",
        "1.   [SFTTrainer](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py)\n",
        "2.   [PEFT](https://github.com/huggingface/peft)\n",
        "3.   [Transformers](https://github.com/huggingface/transformers)\n",
        "4. [PyTorch NN](https://pytorch.org/docs/stable/nn.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSlTBp69PRQP"
      },
      "source": [
        "## ðŸ“š  Create Custom LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dczRnFiOFBkL"
      },
      "outputs": [],
      "source": [
        "from peft.tuners.lora.layer import Linear\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CustomLinearLoRA(Linear):\n",
        "  # This code works for linear layers LoRA such as attention layer.\n",
        "  def update_layer(\n",
        "      self,\n",
        "      adapter_name,\n",
        "      r,\n",
        "      lora_alpha,\n",
        "      lora_dropout,\n",
        "      init_lora_weights,\n",
        "      use_rslora=False,\n",
        "      use_dora=False,\n",
        "  ):\n",
        "    if r <= 0:\n",
        "      raise ValueError(\n",
        "          f\"`r` should be a positive integer value but the value passed is {r}\"\n",
        "      )\n",
        "\n",
        "    self.r[adapter_name] = r\n",
        "    self.lora_alpha[adapter_name] = lora_alpha\n",
        "\n",
        "    # EXERCISE 17: Define a dropout layer\n",
        "    lora_dropout_layer = (\n",
        "        nn.Dropout(p=lora_dropout) if lora_dropout > 0.0 else nn.Identity()\n",
        "    )\n",
        "\n",
        "    self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n",
        "\n",
        "    # Actual trainable parameters\n",
        "    # EXERCISE 18: write a linear layer that goes from self.in_features to r\n",
        "    self.lora_A[adapter_name] = nn.Linear(self.in_features, r, bias=False)\n",
        "    # EXERCISE 19: write a linear layer that goes from r to self.out_features\n",
        "    self.lora_B[adapter_name] = nn.Linear(r, self.out_features, bias=False)\n",
        "\n",
        "    self.scaling[adapter_name] = lora_alpha / r\n",
        "\n",
        "    self.reset_lora_parameters(adapter_name, init_lora_weights)\n",
        "    self.set_adapter(self.active_adapter)\n",
        "\n",
        "  def forward(self, x, *args, **kwargs):\n",
        "    result = self.base_layer(x, *args, **kwargs)\n",
        "    torch_result_dtype = result.dtype\n",
        "\n",
        "    if self.active_adapter not in self.lora_A.keys():\n",
        "      return result\n",
        "    lora_A = self.lora_A[self.active_adapter]\n",
        "    lora_B = self.lora_B[self.active_adapter]\n",
        "    dropout = self.lora_dropout[self.active_adapter]\n",
        "    scaling = self.scaling[self.active_adapter]\n",
        "\n",
        "    x = x.to(lora_A.weight.dtype)\n",
        "\n",
        "    x = dropout(x)\n",
        "\n",
        "    # EXERCISE 20: add to the result of the base layer, the output of\n",
        "    # lora_B and lora_A and multiply by the scaling factor\n",
        "    result = result + lora_B(lora_A(x)) * scaling\n",
        "\n",
        "    result = result.to(torch_result_dtype)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiXUQT4EHor6"
      },
      "source": [
        "## Hooking PeFT Model with Huggingface [Purely orchestration code]\n",
        "\n",
        "Since we are using the HuggingFace PEFT library framework, we need to tweak some of its internal workings to be able to expose the LoRA layer above. Therefore the following cell is not very insightful to understand during this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTGy0A06FU2z"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, LoraModel, PeftModel, get_peft_model\n",
        "from peft.tuners.lora.layer import Linear, dispatch_default\n",
        "from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers.utils import PushToHubMixin\n",
        "\n",
        "\n",
        "def custom_dispatch_default(\n",
        "    target: torch.nn.Module, adapter_name, lora_config, **kwargs\n",
        "):\n",
        "  new_module = None\n",
        "  target_base_layer = (\n",
        "      target.get_base_layer() if isinstance(target, BaseTunerLayer) else target\n",
        "  )\n",
        "\n",
        "  if isinstance(target_base_layer, torch.nn.Linear):\n",
        "    kwargs.update(lora_config.loftq_config)\n",
        "    new_module = CustomLinearLoRA(target, adapter_name, **kwargs)\n",
        "\n",
        "  if new_module is None:\n",
        "    new_module = dispatch_default(\n",
        "        target, adapter_name, lora_config=lora_config, **kwargs\n",
        "    )\n",
        "  return new_module\n",
        "\n",
        "\n",
        "class CustomLoraModel(LoraModel):\n",
        "\n",
        "  @staticmethod\n",
        "  def _create_new_module(lora_config, adapter_name, target, **kwargs):\n",
        "    return custom_dispatch_default(\n",
        "        target, adapter_name, lora_config=lora_config, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "class CustomPeftModel(PeftModel):\n",
        "\n",
        "  def __init__(self, model, peft_config, adapter_name=\"default\"):\n",
        "    PushToHubMixin.__init__(self)\n",
        "    torch.nn.Module.__init__(self)\n",
        "\n",
        "    self.modules_to_save = None\n",
        "    self.active_adapter = adapter_name\n",
        "    self.peft_type = peft_config.peft_type\n",
        "    # These args are special PEFT arguments that users can pass.\n",
        "    # They need to be removed before passing them to forward.\n",
        "    self.special_peft_forward_args = {\"adapter_names\"}\n",
        "\n",
        "    self._is_prompt_learning = peft_config.is_prompt_learning\n",
        "    self._peft_config = None\n",
        "    self.base_model = CustomLoraModel(\n",
        "        model, {adapter_name: peft_config}, adapter_name\n",
        "    )\n",
        "\n",
        "    self.set_additional_trainable_modules(peft_config, adapter_name)\n",
        "\n",
        "    if getattr(model, \"is_gradient_checkpointing\", True):\n",
        "      model = self._prepare_model_for_gradient_checkpointing(model)\n",
        "\n",
        "    # the `pretraining_tp` is set for some models to simulate Tensor\n",
        "    # Parallelism during inference to avoid numerical differences,\n",
        "    # https://github.com/pytorch/pytorch/issues/76232 - to avoid any unexpected\n",
        "    # behavior we disable that in this line.\n",
        "    if hasattr(self.base_model, \"config\") and hasattr(\n",
        "        self.base_model.config, \"pretraining_tp\"\n",
        "    ):\n",
        "      self.base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bCasYVRHwKY"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxQKk5fbFZ2Z"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "MODEL = reload_model()\n",
        "MODEL = CustomPeftModel(MODEL, lora_config)\n",
        "\n",
        "# Use the Learning Rate the same as from Step 10 full LoRA Run\n",
        "trainer = train(\n",
        "    model=MODEL,\n",
        "    lora_config=lora_config,\n",
        "    learning_rate=LORA_LEARNING_RATE,\n",
        "    num_epochs=2,\n",
        ")\n",
        "print(print_trainable_parameters(MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8IfM-uNIX1z"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lGDPVJ7IdfE"
      },
      "outputs": [],
      "source": [
        "run_ft_evals()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBI0f4lYdQ5R"
      },
      "source": [
        "# Other PEFT Techniques\n",
        "\n",
        "\n",
        "1.   Quantization [doc](https://huggingface.co/docs/peft/en/developer_guides/quantization)\n",
        "2.   Quantization + LoRA [doc](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
        "3.   Soft prompt tuning [doc](https://huggingface.co/docs/peft/en/conceptual_guides/prompting)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu98-_ZlGpQZ"
      },
      "source": [
        "# âœ¨ Resources used for this tutorial and references\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. [Huggingface LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
        "2.   [Huggingface Finetuning](https://huggingface.co/docs/transformers/en/training)\n",
        "3. [LLM Optimizers](https://towardsdatascience.com/fine-tuning-llms-with-32-bit-8-bit-and-paged-adamw-optimizers-1034e3105634)\n",
        "4. [LoRA Hypers](https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133)\n",
        "5. [M2Lschool Previous Tutorial](https://github.com/M2Lschool/tutorials2024/blob/main/1_nlp/part_III_llm_finetuning/LoRA.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_4jsyPm6PGyS",
        "uiXUQT4EHor6"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}