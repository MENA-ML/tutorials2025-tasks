# Practical sessions @ MENA ML

## 1. Transformers/LLMs
*Authors: Ibrahim Alabdulmohsin and Mehdi Bennani*

This tutorial provides a hands-on introduction to PyTorch, a popular deep learning framework, and guides you through building a Transformer model from scratch. Its first part is dedicated to fundamental PyTorch concepts, including tensor operations, automatic differentiation, and building basic deep learning architectures. The second part of this tutorial delves into Natural Language Processing (NLP) by introducing text tokenization and demonstrating how to build a tokenizer. Finally, the last two sections of this practical guides you through implementing a Transformer model for classification and sequence generation tasks.

## 2. Efficient LLM training
*Author: Pranali Yawalkar*

This tutorial explores the fundamental aspects of Finetuning Large Language Models for specific tasks or domains. Prior knowledge of standard LLM components (e.g. transformers, autoregression) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge. This tutorial combines detailed analysis and development of essential LLM Finetuning concepts via a practical exercise. Other necessary components will be developed using PyTorch. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications.

**Prerequests:
Please follow the instructions in the corresponding notebook to set up a Hugginface account prior to the session.**

## 3. Reinforcement Learning from Human Feedback
*Author: Yasin Abbasi Yadkori*

This tutorial demonstrates how to fine-tune an LLM using Direct Preference Optimization. The goal is to guide the model towards generating stories with specific characteristics, in this case, children's stories, by leveraging a dataset of preferred and rejected story examples. In this tutorial, you will explore the concept of Reinforcement Learning, work with pre-trained LLMs, and evaluate the impact of DPO on model's performance.
